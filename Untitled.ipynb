{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "563c6edf-9a2e-4507-8786-0706538566da",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5780/981942182.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "#Descrição de cada Biblioteca:\n",
    "#Pandas, utilizada para a organização dos dados em tabelas\n",
    "import pandas as pd\n",
    "\n",
    "import codecs\n",
    "import sys\n",
    "\n",
    "#os, utilizada para ler arquivos no diretório que sirvam como banco de dados, no caso deste trabalho, foram utilizados arquivos .csv\n",
    "import os\n",
    "\n",
    "#re, biblioteca para uso de expressões regulares\n",
    "import re\n",
    "\n",
    "#spacy, biblioteca de processamento de linguagem natural\n",
    "import spacy as spy\n",
    "#simport sys\n",
    "\n",
    "#gensim, biblioteca utilizada para implementação do método para criação do vocabulário, modelagem de tópicos LDA\n",
    "from gensim import corpora\n",
    "import gensim as gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87e42f0c-7779-4c54-a6d3-16a756128912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%time\n",
    "#Tratamento de caracteres\n",
    "directory = \"./FakeWhatsApp.BrMeu/data/2018\"\n",
    "ignoredFiles = [\".ipynb_checkpoints\", \"users.csv\", \"user_features.csv\", \n",
    "                \"preprocessed_corpus.p.pickle\", \"train_user_features.npy\", \"grouped_texts.pickle\",\n",
    "               \"wpp_labels - original midia.csv\", \"train-test\", \"word2vec.model\",\"edges.p\",\"nodes.p\",\n",
    "               \"test_user_features.npy\", \"fakeWhatsApp.BR_2018_preprocessed_texts.csv\", \n",
    "               \"fakeWhatsApp.BR_2018_propagation.csv\", \"fakeWhatsApp.BR_2018.csv\"]\n",
    "mensagens=[]\n",
    "mensagensToLower=[]\n",
    "\n",
    "#Trata os termos dentro dos folders obtidos anteAssertionErrorriormente, e os coloca dentro de um array\n",
    "def lerFolder():\n",
    "    for filename in os.listdir(directory):\n",
    "        readFileBool = True\n",
    "        #if filename != \".ipynb_checkpoints\" and filename != \"users.csv\" and filename != \"user_features.csv\" and filename != \"preprocessed_corpus.p.pickle\" and filename != \"train_user_features.npy\" and filename != \"grouped_texts.pickle\" and filename != \"wpp_labels - original midia.csv\" and filename != \"train-test\" and filename != \"word2vec.model\" and filename != \"edges.p\" and filename != \"nodes.p\" and filename != \"test_user_features.npy\":        \n",
    "            #print(filename)\n",
    "        \n",
    "        for file in ignoredFiles:\n",
    "           if filename == file:\n",
    "                readFileBool = False\n",
    "                break\n",
    "           #readFileBool = True\n",
    "            \n",
    "        if readFileBool == True:\n",
    "            colCsv = ''\n",
    "            \n",
    "            csvRead = pd.read_csv(directory+\"/\"+filename)\n",
    "            for col in csvRead.columns:\n",
    "                if col == 'text':\n",
    "                    colCsv = 'text'\n",
    "                    break\n",
    "                if col == 'preprocessed_text':\n",
    "                    colCsv = 'preprocessed_text'\n",
    "                    break\n",
    "\n",
    "            for data in csvRead[colCsv]:\n",
    "                mensagens.append(data)\n",
    "            readFileBool = False\n",
    "\n",
    "#Trata todos os textos dentro dos arquivos os transformando para minúsculo\n",
    "def letrasMinusculas():\n",
    "    for msg in mensagens:\n",
    "        mensagensToLower.append(msg.lower())\n",
    "\n",
    "#Remove o tweets com caracteres menores que 5\n",
    "def removerMsgsPoucosCaracteres():\n",
    "    #tweetsMenores = 0\n",
    "    for item in mensagensToLower:\n",
    "        if (len(item) < 5):\n",
    "            mensagensToLower.remove(item)\n",
    "    #print(\"tweets menores que 5: \"+str(tweetsMenores))\n",
    "\n",
    "#Remove links, citações, hastags e caracteres que são símbolos\n",
    "def removerLinkQuotesHashtags():  \n",
    "    #contadorLinks = 0\n",
    "    #contadorHashTags = 0\n",
    "    #contadorCaracteres = 0\n",
    "    #contadorCitacoes = 0\n",
    "    #Remove links nas seguintes estruturas:\n",
    "    #https://subdom.dom.com.countrycode/urlstr\n",
    "    #https://subdom.dom.com/urlstr\n",
    "    #https://subdom.dom/urlstr\n",
    "    for item in range(len(mensagensToLower)):\n",
    "        if len(re.findall(\"https://[a-zA-Z0-9]+[.][a-zA-Z0-9]+[.][a-zA-Z0-9]+[.][a-zA-Z0-9]+[/[a-zA-Z0-9]+]*\", mensagensToLower[item])) > 0:\n",
    "            mensagensToLower[item]=re.sub(\"https://[a-zA-Z0-9]+[.][a-zA-Z0-9]+[.][a-zA-Z0-9]+[.][a-zA-Z0-9]+[/[a-zA-Z0-9]+]*\",\"\", mensagensToLower[item])\n",
    "            #print(tweetsToLower[item])\n",
    "            #contadorLinks += 1\n",
    "        if len(re.findall(\"https://[a-zA-Z0-9]+[.][a-zA-Z0-9]+[.][a-zA-Z0-9]+[/[a-zA-Z0-9]+]*\", mensagensToLower[item])) > 0:\n",
    "            mensagensToLower[item]=re.sub(\"https://[a-zA-Z0-9]+[.][a-zA-Z0-9]+[.][a-zA-Z0-9]+[/[a-zA-Z0-9]+]*\",\"\", mensagensToLower[item])\n",
    "            #print(tweetsToLower[item])\n",
    "            #contadorLinks += 1\n",
    "        if len(re.findall(\"https://[a-zA-Z0-9]+[.][a-zA-Z0-9]+[/[a-zA-Z0-9]+]*\", mensagensToLower[item])) > 0:\n",
    "            mensagensToLower[item]=re.sub(\"https://[a-zA-Z0-9]+[.][a-zA-Z0-9]+[/[a-zA-Z0-9]+]*\",\"\", mensagensToLower[item])\n",
    "            #contadorLinks += 1\n",
    "            #print(tweetsToLower[item])\n",
    "    \n",
    "    #Remove # no inicio e final de palavras\n",
    "    for item in range(len(mensagensToLower)):\n",
    "        if len(re.findall(\"#[a-zA-Z0-9]+\", mensagensToLower[item])) > 0: \n",
    "            #print(re.findall(\"#[a-zA-Z0-9]+\", tweetsToLower[item]))\n",
    "            mensagensToLower[item] = re.sub(\"#[a-zA-Z0-9]+\", \"\", mensagensToLower[item])\n",
    "            #contadorHashTags += 1\n",
    "        if len(re.findall(\"[a-zA-Z0-9]+#\", mensagensToLower[item])) > 0:\n",
    "            #print(re.findall(\"[a-zA-Z0-9]+#\", tweetsToLower[item])):\n",
    "            mensagensToLower[item] = re.sub(\"[a-zA-Z0-9]+#\", \"\", mensagensToLower[item])\n",
    "            #contadorHashTags += 1\n",
    "            \n",
    "    #Remove caracteres não alfanuméricos, no inicio, meio e fim de sentenças\n",
    "    for item in range(len(mensagensToLower)):\n",
    "        if len(re.findall(\"[a-zA-Z0-9]+[@:!$%&*+=|]+[a-zA-Z0-9]+\", mensagensToLower[item])) > 0:\n",
    "            #contadorCaracteres += 1\n",
    "            for iter in re.findall(\"[a-zA-Z0-9]+[@:!$%&*+=|]+[a-zA-Z0-9]+\", mensagensToLower[item]):\n",
    "                mensagensToLower[item] = re.sub(\"[a-zA-Z0-9]+[@:!$%&*+=|]+[a-zA-Z0-9]+\",\"\", iter)\n",
    "                \n",
    "        if len(re.findall(\"[:!$%&*+=|]+[a-zA-Z0-9]+\", mensagensToLower[item])) > 0:\n",
    "            #contadorCaracteres += 1\n",
    "            #print(re.findall(\"[@:!$%&*+=|][a-zA-Z0-9]+\", tweetsToLower[item]))\n",
    "            for iter in re.findall(\"[:!$%&*+=|]+[a-zA-Z0-9]+\", mensagensToLower[item]):\n",
    "                mensagensToLower[item] = re.sub(\"[:!$%&*+=|]+[a-zA-Z0-9]+\",\"\", iter)\n",
    "                \n",
    "        if len(re.findall(\"[a-zA-Z0-9]+[@:!$%&*+=|]\", mensagensToLower[item])) > 0:\n",
    "            #contadorCaracteres += 1\n",
    "            #print(re.findall(\"[a-zA-Z0-9]+[@:!$%&*+=|]\", tweetsToLower[item]))\n",
    "            for iter in re.findall(\"[a-zA-Z0-9]+[@:!$%&*+=|]\", mensagensToLower[item]):\n",
    "                mensagensToLower[item] = re.sub(\"[a-zA-Z0-9]+[@:!$%&*+=|]\",\"\", iter)\n",
    "\n",
    "    #Remove citações de usuários\n",
    "    for item in range(len(mensagensToLower)):\n",
    "        if len(re.findall(\"[@]+[a-zA-Z0-9]+\", mensagensToLower[item])) > 0:\n",
    "            #contadorCitacoes += 1\n",
    "            #print(re.findall(\"[@:!$%&*+=|][a-zA-Z0-9]+\", tweetsToLower[item]))\n",
    "            for iter in re.findall(\"[@]+[a-zA-Z0-9]+\", mensagensToLower[item]):\n",
    "                mensagensToLower[item] = re.sub(\"[@]+[a-zA-Z0-9]+\",\"\", iter)\n",
    "   # print(\"links removidos: \"+ str(contadorLinks))\n",
    "   # print(\"hashtags removidos: \"+ str(contadorHashTags))\n",
    "   # print(\"caracteres não alfanuméricos removidos: \"+ str(contadorCaracteres))\n",
    "   # print(\"citacoes removidas: \"+ str(contadorCitacoes))\n",
    "\n",
    "def tratarCaracteresEPalavras():\n",
    "    letrasMinusculas()\n",
    "    removerMsgsPoucosCaracteres()\n",
    "    removerLinkQuotesHashtags()\n",
    "    #print(mensagensToLower)\n",
    "\n",
    "lerFolder()\n",
    "tratarCaracteresEPalavras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83529350-2b92-45b0-a0f7-431f8dafd9f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Tokenização\n",
    "msgsTokens = []\n",
    "#Crias os tokens\n",
    "def criarTokens():\n",
    "    for msg in mensagensToLower:\n",
    "        msgsTokens.append(msg.split())\n",
    "\n",
    "#ELiminar tokens pequenos com menos de 3 de comprimento\n",
    "def eliminarTokensEPequenos():\n",
    "    #tokensPequenosEliminados = 0\n",
    "    #msgAlterada = False\n",
    "    for msg in msgsTokens:\n",
    "        #print(token)\n",
    "        for item in msg:\n",
    "            if len(item) < 3:\n",
    "                #msgAlterada = True\n",
    "                msg.remove(item)\n",
    "        #if msgAlterada == True:\n",
    "            #tokensPequenosEliminados += 1\n",
    "         #   msgAlterada = False\n",
    "    #print(tokensPequenosEliminados)\n",
    "\n",
    "#Elimina as stopwords presentes no array stopWords\n",
    "def elimiarStopWords():\n",
    "    #stopWordsEliminadas = 0\n",
    "   # msgAlterada = False\n",
    "    stopWords = ['que', 'para', 'com',  'não',  'uma', 'por',  'mais',  'dos',  'como',  'mas',  'foi', 'ele',  'das',  'tem', 'seu',  'sua', 'ser',  'quando',  'muito', 'nos', 'está', 'também', 'pelo',  'pela',  'até',  'isso',  'ela',  'entre',  'era',  'depois',  'sem',  'mesmo',  'aos',  'ter',  'seus',  'quem',  'nas',  'esse',  'eles',  'estão',  'você',  'tinha',  'foram',  'essa',  'num',  'nem',  'suas',  'meu', 'minha',  'têm',  'numa',  'pelos',  'elas',  'havia',  'seja',  'qual',  'será',  'nós',  'tenho',  'lhe',  'deles',  'essas',  'esses',  'pelas',  'este',  'fosse',  'dele', 'vocês',  'vos',  'lhes',  'meus',  'minhas',  'teu',  'tua',  'teus',  'tuas',  'nosso',  'nossa',  'nossos',  'nossas',  'dela',  'delas',  'esta',  'estes',  'estas',  'aquele',  'aquela',  'aqueles',  'aquelas',  'isto',  'aquilo',  'estou',  'está',  'estamos',  'estão',  'estive',  'esteve',  'estivemos',  'estiveram',  'estava',  'estávamos',  'estavam',  'estivera',  'estivéramos',  'esteja',  'estejamos',  'estejam',  'estivesse',  'estivéssemos',  'estivessem',  'estiver',  'estivermos',  'estiverem',  'hei', 'havemos',  'hão',  'houve',  'houvemos',  'houveram',  'houvera',  'houvéramos',  'haja',  'hajamos',  'hajam',  'houvesse',  'houvéssemos',  'houvessem',  'houver',  'houvermos',  'houverem',  'houverei',  'houverá',  'houveremos',  'houverão',  'houveria',  'houveríamos',  'houveriam',  'sou',  'somos',  'são',  'era',  'éramos',  'eram',  'fui',  'foi',  'fomos',  'foram',  'fora',  'fôramos',  'seja',  'sejamos',  'sejam',  'fosse',  'fôssemos',  'fossem',  'for',  'formos',  'forem',  'serei',  'será',  'seremos',  'serão',  'seria',  'seríamos',  'seriam',  'tenho',  'tem',  'temos',  'tém',  'tinha',  'tínhamos',  'tinham',  'tive',  'teve',  'tivemos',  'tiveram',  'tivera',  'tivéramos',  'tenha',  'tenhamos',  'tenham',  'tivesse',  'tivéssemos',  'tivessem',  'tiver',  'tivermos',  'tiverem',  'terei',  'terá',  'teremos',  'terão',  'teria',  'teríamos',  'teriam']\n",
    "    for msg in msgsTokens:\n",
    "        for item in msg:\n",
    "            for word in stopWords:\n",
    "                if item == word:\n",
    "                    try:\n",
    "           #             tweeAlterado = True\n",
    "                        msg.remove(item)\n",
    "                    except:\n",
    "                        continue\n",
    "        #if tweeAlterado == True:\n",
    "         #   stopWordsEliminadas += 1\n",
    "          #  tweeAlterado = False\n",
    "            \n",
    "#Elimia os tokens vazios, len(token) == 0\n",
    "def eliminarTokensVazios():\n",
    "    for msg in msgsTokens:\n",
    "        if len(msg) == 0:\n",
    "            msgsTokens.remove(msg)\n",
    "\n",
    "def tokenizacao():\n",
    "    criarTokens()\n",
    "    eliminarTokensEPequenos()\n",
    "    elimiarStopWords()\n",
    "    eliminarTokensVazios()\n",
    "#    print(msgsTokens)\n",
    "tokenizacao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adcf7a99-f0e4-4fd2-aeed-9791fb2a1a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inicio do processo de modelagem de tópicos LDA\n",
    "#Continuação do processo\n",
    "dictionary = corpora.Dictionary(msgsTokens)\n",
    "corpus = [dictionary.doc2bow(textos) for textos in msgsTokens]\n",
    "ldamodel = gs.models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=5)\n",
    "topics3 = ldamodel.print_topics(num_topics=6, num_words=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e5a839b2-d031-441f-92f8-a7d7d9e8352a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Limpeza dos dados obtidos pelo LDA\n",
    "cleanedVocab = []\n",
    "\n",
    "#Lematização com spacy\n",
    "#Para utilizar a biblioteca, é necessário baixar o modelo para a lingua portuguesa\n",
    "#python -m spacy download pt_core_news_sm\n",
    "def lematizar():\n",
    "    #Carregamento do modelo\n",
    "    nlp = spy.load(\"pt_core_news_sm\")\n",
    "    for num in range(len(cleanedVocab)):\n",
    "       for item in range(len(cleanedVocab[num])):\n",
    "           #Processo de lematização de cada palavra presente em ttweetsTokens\n",
    "           tokenItem = nlp(cleanedVocab[num][item])\n",
    "           #print(tokenItem)\n",
    "           cleanedVocab[num][item] = tokenItem\n",
    "#Tira as probabilidade e e o sinal de '*'\n",
    "def limparTopicos():\n",
    "    for tuple in topics3:\n",
    "        for element in range(len(tuple)):\n",
    "            if tuple[element] == tuple[0]:\n",
    "                continue\n",
    "            if re.match(\"0[.0-9][0-9][0-9*]\",tuple[element]):\n",
    "                cleanedVocab.append(re.sub(\"0[.0-9][0-9][0-9][0-9][*]\",\"\",tuple[element]))\n",
    "\n",
    "#Tira as aspas duplas e o sinal de '+'\n",
    "def limparVocabulario():    \n",
    "    for element in range(len(cleanedVocab)):\n",
    "            #print(cleanedVocab[element])\n",
    "            if re.findall(r'[\"]', cleanedVocab[element]):\n",
    "                cleanedVocab[element] = re.sub(r'[\"]',\"\", cleanedVocab[element])\n",
    "            if re.findall(\"[/s+/s]\", cleanedVocab[element]):\n",
    "                cleanedVocab[element] = re.sub(r'[/s+/s]',\"\", cleanedVocab[element])\n",
    "                \n",
    "#Divide os tópicos de cada elemento do array, baseado nos espaços entre os tópicos \n",
    "def dividirArray():\n",
    "    for element in range(len(cleanedVocab)):\n",
    "        cleanedVocab[element] = cleanedVocab[element].split(\" \")\n",
    "\n",
    "#Por fim retorna o dataFrame com o vocabulário\n",
    "def dataFrameVocab():\n",
    "    vocab = []\n",
    "    #Esta variável serve para monitorar se existem repetições de palavras achadas pelo LDA\n",
    "    vocabAdd = True\n",
    "    for array in cleanedVocab:\n",
    "        for element in array:\n",
    "            #Caso a iteração ache um '', este não é adicionado\n",
    "            if element == '' or element == \"<arquivo\" or element == \"oculto>\":\n",
    "                continue\n",
    "            for item in vocab:\n",
    "                if item == element:\n",
    "                    vocabAdd = False\n",
    "                    break\n",
    "            #Se vocabAdd = True adicionar o dito elemento\n",
    "            if vocabAdd == True:\n",
    "                vocab.append(element)\n",
    "            else:\n",
    "                pass\n",
    "    #print(vocab)\n",
    "    dataFrameVocab = pd.DataFrame(vocab, columns=[\"Vocabulário\", \"Outra Coluna\"])\n",
    "    return dataFrameVocab    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aeb09c1d-73b5-496a-b65c-3b41aa579444",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def dataFrameVocab():\n",
    "    vocab = []\n",
    "    #Esta variável serve para monitorar se existem repetições de palavras achadas pelo LDA\n",
    "    vocabAdd = True\n",
    "    for array in cleanedVocab:\n",
    "        for element in array:\n",
    "            #Caso a iteração ache um '', este não é adicionado\n",
    "            if element == '' or element == None:\n",
    "                continue\n",
    "                \n",
    "            for item in vocab:\n",
    "                if item == element:\n",
    "                    vocabAdd = False\n",
    "                    break\n",
    "            #Se vocabAdd = True adicionar o dito elemento\n",
    "            print(element)\n",
    "            if vocabAdd == True:\n",
    "                vocab.append(element)\n",
    "            else:\n",
    "                pass\n",
    "    print(vocab)\n",
    "    dataFrameVocab = pd.DataFrame(vocab, columns=[\"Vocabulário\"])\n",
    "    #print()\n",
    "    return dataFrameVocab    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "514a2538-68e3-4f34-8c46-ea7678d5cebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vocabulário</th>\n",
       "      <th>Outra Coluna</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1⃣7⃣</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kkk</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fake</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mídia</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;</td>\n",
       "      <td>arquivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>oculto</td>\n",
       "      <td>&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bem</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>boa</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dar</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pra</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>não</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>vai</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>o</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>bolonaro</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bolonaro</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>kkkkk</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>mito</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Vocabulário Outra Coluna\n",
       "0         1⃣7⃣         None\n",
       "1         None         None\n",
       "2          kkk         None\n",
       "3         None         None\n",
       "4         fake         None\n",
       "5        mídia         None\n",
       "6         None         None\n",
       "7            <      arquivo\n",
       "8         None         None\n",
       "9       oculto            >\n",
       "10         bem         None\n",
       "11        None         None\n",
       "12         boa         None\n",
       "13        None         None\n",
       "14         dar         None\n",
       "15         pra         None\n",
       "16        None         None\n",
       "17         não         None\n",
       "18        None         None\n",
       "19         vai         None\n",
       "20           o         None\n",
       "21        None         None\n",
       "22    bolonaro         None\n",
       "23        None         None\n",
       "24           a         None\n",
       "25    bolonaro         None\n",
       "26        None         None\n",
       "27       kkkkk         None\n",
       "28        None         None\n",
       "29        mito         None"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Criação do vocabulário\n",
    "limparTopicos()\n",
    "limparVocabulario()\n",
    "dividirArray()\n",
    "#print(cleanedVocab)\n",
    "lematizar()\n",
    "dataFrameVocab()\n",
    "#print(cleanedVocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c5c2b7-0872-4d5a-9a63-e13b2f12a780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0bae59-0e75-469c-8987-becf0fa9c7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
